{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.5.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.2 MB 2.1 MB/s eta 0:00:01     |████████████████████████▉       | 9.5 MB 2.1 MB/s eta 0:00:02\n",
      "\u001b[?25hCollecting python-dateutil>=2.8.1\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "\u001b[K     |████████████████████████████████| 247 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy>=1.20.3; python_version < \"3.10\"\n",
      "  Downloading numpy-1.23.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.1 MB 2.2 MB/s eta 0:00:01    |█▌                              | 778 kB 2.2 MB/s eta 0:00:08     |███████████████████▋            | 10.4 MB 2.2 MB/s eta 0:00:04\n",
      "\u001b[?25hCollecting pytz>=2020.1\n",
      "  Downloading pytz-2022.6-py2.py3-none-any.whl (498 kB)\n",
      "\u001b[K     |████████████████████████████████| 498 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas) (1.14.0)\n",
      "Installing collected packages: python-dateutil, numpy, pytz, pandas\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.7.3\n",
      "    Not uninstalling python-dateutil at /usr/lib/python3/dist-packages, outside environment /usr\n",
      "    Can't uninstall 'python-dateutil'. No files were found to uninstall.\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2019.3\n",
      "    Not uninstalling pytz at /usr/lib/python3/dist-packages, outside environment /usr\n",
      "    Can't uninstall 'pytz'. No files were found to uninstall.\n",
      "Successfully installed numpy-1.23.4 pandas-1.5.1 python-dateutil-2.8.2 pytz-2022.6\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.6.2-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.4 MB 2.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.0.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (295 kB)\n",
      "\u001b[K     |████████████████████████████████| 295 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
      "\u001b[K     |████████████████████████████████| 965 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyparsing>=2.2.1\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "\u001b[K     |████████████████████████████████| 98 kB 2.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting packaging>=20.0\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "\u001b[K     |████████████████████████████████| 40 kB 2.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pillow>=6.2.0 in /usr/lib/python3/dist-packages (from matplotlib) (7.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.23.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.14.0)\n",
      "Installing collected packages: kiwisolver, cycler, contourpy, fonttools, pyparsing, packaging, matplotlib\n",
      "Successfully installed contourpy-1.0.6 cycler-0.11.0 fonttools-4.38.0 kiwisolver-1.4.4 matplotlib-3.6.2 packaging-21.3 pyparsing-3.0.9\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.1 MB)\n",
      "\u001b[K     |███████████████████████████████▎| 564.6 MB 268 kB/s eta 0:00:51     |████████▏                       | 146.9 MB 2.3 MB/s eta 0:03:08     |███████████████████▏            | 345.3 MB 2.6 MB/s eta 0:01:31     |████████████████████            | 360.4 MB 2.5 MB/s eta 0:01:28     |█████████████████████▎          | 384.9 MB 2.5 MB/s eta 0:01:17     |██████████████████████████      | 468.2 MB 2.5 MB/s eta 0:00:44     |███████████████████████████▍    | 495.1 MB 2.1 MB/s eta 0:00:39"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-09a4b887f633>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "class ForecastLSTM:\n",
    "    def __init__(self, random_seed: int = 1234):\n",
    "        self.random_seed = random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_dataset(self, df: pd.DataFrame) -> np.array:\n",
    "    # y 컬럼을 데이터프레임의 맨 마지막 위치로 이동\n",
    "    if \"y\" in df.columns:\n",
    "        df = df.drop(columns=[\"y\"]).assign(y=df[\"y\"])\n",
    "    else:\n",
    "        raise KeyError(\"Not found target column 'y' in dataset.\")\n",
    "    \n",
    "    # shape 변경\n",
    "    dataset = df.values.reshape(df.shape)\n",
    "    return dataset\n",
    "\n",
    "ForecastLSTM.reshape_dataset = reshape_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequences(\n",
    "    self, dataset: np.array, seq_len: int, steps: int, single_output: bool\n",
    ") -> tuple:\n",
    "\n",
    "    # feature와 y 각각 sequential dataset을 반환할 리스트 생성\n",
    "    X, y = list(), list()\n",
    "    # sequence length와 step에 따라 sequential dataset 생성\n",
    "    for i, _ in enumerate(dataset):\n",
    "        idx_in = i + seq_len\n",
    "        idx_out = idx_in + steps\n",
    "        if idx_out > len(dataset):\n",
    "            break\n",
    "        seq_x = dataset[i:idx_in, :-1]\n",
    "        if single_output:\n",
    "            seq_y = dataset[idx_out - 1 : idx_out, -1]\n",
    "        else:\n",
    "            seq_y = dataset[idx_in:idx_out, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "ForecastLSTM.split_sequences = split_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_valid_dataset(\n",
    "    self,\n",
    "    df: pd.DataFrame,\n",
    "    seq_len: int,\n",
    "    steps: int,\n",
    "    single_output: bool,\n",
    "    validation_split: float = 0.3,\n",
    "    verbose: bool = True,\n",
    ") -> tuple:\n",
    "    # dataframe을 numpy array로 reshape\n",
    "    dataset = self.reshape_dataset(df=df)\n",
    "\n",
    "    # feature와 y를 sequential dataset으로 분리\n",
    "    X, y = self.split_sequences(\n",
    "        dataset=dataset,\n",
    "        seq_len=seq_len,\n",
    "        steps=steps,\n",
    "        single_output=single_output,\n",
    "    )\n",
    "\n",
    "    # X, y에서 validation dataset 분리\n",
    "    dataset_size = len(X)\n",
    "    train_size = int(dataset_size * (1 - validation_split))\n",
    "    X_train, y_train = X[:train_size, :], y[:train_size, :]\n",
    "    X_val, y_val = X[train_size:, :], y[train_size:, :]\n",
    "    if verbose:\n",
    "        print(f\" >>> X_train: {X_train.shape}\")\n",
    "        print(f\" >>> y_train: {y_train.shape}\")\n",
    "        print(f\" >>> X_val: {X_val.shape}\")\n",
    "        print(f\" >>> y_val: {y_val.shape}\")\n",
    "    return X_train, y_train, X_val, y_val\n",
    "\n",
    "\n",
    "ForecastLSTM.split_train_valid_dataset = split_train_valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_compile_lstm_model(\n",
    "    self,\n",
    "    seq_len: int,\n",
    "    n_features: int,\n",
    "    lstm_units: list,\n",
    "    learning_rate: float,\n",
    "    dropout: float,\n",
    "    steps: int,\n",
    "    metrics: str,\n",
    "    single_output: bool,\n",
    "    last_lstm_return_sequences: bool = False,\n",
    "    dense_units: list = None,\n",
    "    activation: str = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    LSTM 네트워크를 생성한 결과를 반환한다.\n",
    "\n",
    "    :param seq_len: Length of sequences. (Look back window size)\n",
    "    :param n_features: Number of features. It requires for model input shape.\n",
    "    :param lstm_units: Number of cells each LSTM layers.\n",
    "    :param learning_rate: Learning rate.\n",
    "    :param dropout: Dropout rate.\n",
    "    :param steps: Length to predict.\n",
    "    :param metrics: Model loss function metric.\n",
    "    :param single_output: Whether 'yhat' is a multiple value or a single value.\n",
    "    :param last_lstm_return_sequences: Last LSTM's `return_sequences`. Allow when `single_output=False` only.\n",
    "    :param dense_units: Number of cells each Dense layers. It adds after LSTM layers.\n",
    "    :param activation: Activation function of Layers.\n",
    "    \"\"\"\n",
    "    tf.random.set_seed(self.random_seed)\n",
    "    model = Sequential()\n",
    "\n",
    "    if len(lstm_units) > 1:\n",
    "        # LSTM -> ... -> LSTM -> Dense(steps)\n",
    "        model.add(\n",
    "            LSTM(\n",
    "                units=lstm_units[0],\n",
    "                activation=activation,\n",
    "                return_sequences=True,\n",
    "                input_shape=(seq_len, n_features),\n",
    "            )\n",
    "        )\n",
    "        lstm_layers = lstm_units[1:]\n",
    "        for i, n_units in enumerate(lstm_layers, start=1):\n",
    "            if i == len(lstm_layers):\n",
    "                if single_output:\n",
    "                    return_sequences = False\n",
    "                else:\n",
    "                    return_sequences = last_lstm_return_sequences\n",
    "                model.add(\n",
    "                    LSTM(\n",
    "                        units=n_units,\n",
    "                        activation=activation,\n",
    "                        return_sequences=return_sequences,\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                model.add(\n",
    "                    LSTM(\n",
    "                        units=n_units,\n",
    "                        activation=activation,\n",
    "                        return_sequences=True,\n",
    "                    )\n",
    "                )\n",
    "    else:\n",
    "        # LSTM -> Dense(steps)\n",
    "        if single_output:\n",
    "            return_sequences = False\n",
    "        else:\n",
    "            return_sequences = last_lstm_return_sequences\n",
    "        model.add(\n",
    "            LSTM(\n",
    "                units=lstm_units[0],\n",
    "                activation=activation,\n",
    "                return_sequences=return_sequences,\n",
    "                input_shape=(seq_len, n_features),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if single_output:  # Single Step, Direct Multi Step\n",
    "        if dense_units:\n",
    "            for n_units in dense_units:\n",
    "                model.add(Dense(units=n_units, activation=activation))\n",
    "        if dropout > 0:\n",
    "            model.add(Dropout(rate=dropout))\n",
    "        model.add(Dense(1))\n",
    "    else:  # Multiple Output Step\n",
    "        if last_lstm_return_sequences:\n",
    "            model.add(Flatten())\n",
    "        if dense_units:\n",
    "            for n_units in dense_units:\n",
    "                model.add(Dense(units=n_units, activation=activation))\n",
    "        if dropout > 0:\n",
    "            model.add(Dropout(rate=dropout))\n",
    "        model.add(Dense(units=steps))\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=MSE, metrics=metrics)\n",
    "    return model\n",
    "\n",
    "\n",
    "ForecastLSTM.build_and_compile_lstm_model = build_and_compile_lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f6e0e47b6204>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m def fit_lstm(\n\u001b[1;32m      2\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlstm_units\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "def fit_lstm(\n",
    "    self,\n",
    "    df: pd.DataFrame,\n",
    "    steps: int,\n",
    "    lstm_units: list,\n",
    "    activation: str,\n",
    "    dropout: float = 0,\n",
    "    seq_len: int = 16,\n",
    "    single_output: bool = False,\n",
    "    epochs: int = 200,\n",
    "    batch_size: int = None,\n",
    "    steps_per_epoch: int = None,\n",
    "    learning_rate: float = 0.001,\n",
    "    patience: int = 10,\n",
    "    validation_split: float = 0.3,\n",
    "    last_lstm_return_sequences: bool = False,\n",
    "    dense_units: list = None,\n",
    "    metrics: str = \"mse\",\n",
    "    check_point_path: str = None,\n",
    "    verbose: bool = False,\n",
    "    plot: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    LSTM 기반 모델 훈련을 진행한다.\n",
    "\n",
    "    :param df: DataFrame for model train.\n",
    "    :param steps: Length to predict.\n",
    "    :param lstm_units: LSTM, Dense Layers\n",
    "    :param activation: Activation function for LSTM, Dense Layers.\n",
    "    :param dropout: Dropout ratio between Layers.\n",
    "    :param seq_len: Length of sequences. (Look back window size)\n",
    "    :param single_output: Select whether 'y' is a continuous value or a single value.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(self.random_seed)\n",
    "    tf.random.set_seed(self.random_seed)\n",
    "\n",
    "    # 훈련, 검증 데이터셋 생성\n",
    "    (\n",
    "        self.X_train,\n",
    "        self.y_train,\n",
    "        self.X_val,\n",
    "        self.y_val,\n",
    "    ) = self.split_train_valid_dataset(\n",
    "        df=df,\n",
    "        seq_len=seq_len,\n",
    "        steps=steps,\n",
    "        validation_split=validation_split,\n",
    "        single_output=single_output,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    # LSTM 모델 생성\n",
    "    n_features = df.shape[1] - 1\n",
    "    self.model = self.build_and_compile_lstm_model(\n",
    "        seq_len=seq_len,\n",
    "        n_features=n_features,\n",
    "        lstm_units=lstm_units,\n",
    "        activation=activation,\n",
    "        learning_rate=learning_rate,\n",
    "        dropout=dropout,\n",
    "        steps=steps,\n",
    "        last_lstm_return_sequences=last_lstm_return_sequences,\n",
    "        dense_units=dense_units,\n",
    "        metrics=metrics,\n",
    "        single_output=single_output,\n",
    "    )\n",
    "\n",
    "    # 모델 적합 과정에서 best model 저장\n",
    "    if check_point_path is not None:\n",
    "        # create checkpoint\n",
    "        checkpoint_path = f\"checkpoint/lstm_{check_point_path}.h5\"\n",
    "        checkpoint = ModelCheckpoint(\n",
    "            filepath=checkpoint_path,\n",
    "            save_weights_only=False,\n",
    "            save_best_only=True,\n",
    "            monitor=\"val_loss\",\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        rlr = ReduceLROnPlateau(\n",
    "            monitor=\"val_loss\", factor=0.5, patience=patience, verbose=verbose\n",
    "        )\n",
    "        callbacks = [checkpoint, EarlyStopping(patience=patience), rlr]\n",
    "    else:\n",
    "        rlr = ReduceLROnPlateau(\n",
    "            monitor=\"val_loss\", factor=0.5, patience=patience, verbose=verbose\n",
    "        )\n",
    "        callbacks = [EarlyStopping(patience=patience), rlr]\n",
    "\n",
    "    # 모델 훈련\n",
    "    self.history = self.model.fit(\n",
    "        self.X_train,\n",
    "        self.y_train,\n",
    "        batch_size=batch_size,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=(self.X_val, self.y_val),\n",
    "        epochs=epochs,\n",
    "        use_multiprocessing=True,\n",
    "        workers=8,\n",
    "        verbose=verbose,\n",
    "        callbacks=callbacks,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    # 훈련 종료 후 best model 로드\n",
    "    if check_point_path is not None:\n",
    "        self.model.load_weights(f\"checkpoint/lstm_{check_point_path}.h5\")\n",
    "\n",
    "    # 모델링 과정 시각화\n",
    "    if plot:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(self.history.history[f\"{metrics}\"])\n",
    "        plt.plot(self.history.history[f\"val_{metrics}\"])\n",
    "        plt.title(\"Performance Metric\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(f\"{metrics}\")\n",
    "        if metrics == \"mape\":\n",
    "            plt.axhline(y=10, xmin=0, xmax=1, color=\"grey\", ls=\"--\", alpha=0.5)\n",
    "        plt.legend([\"Train\", \"Validation\"], loc=\"upper right\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "ForecastLSTM.fit_lstm = fit_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_validation_dataset(self) -> pd.DataFrame:\n",
    "    # 검증 데이터셋의 실제 값(y)과, 예측 값(yhat)을 저장할 리스트 생성\n",
    "    y_pred_list, y_val_list = list(), list()\n",
    "    \n",
    "    # 훈련된 모델로 validation dataset에 대한 예측값 생성\n",
    "    for x_val, y_val in zip(self.X_val, self.y_val):\n",
    "        x_val = np.expand_dims(\n",
    "            x_val, axis=0\n",
    "        )  # (seq_len, n_features) -> (1, seq_len, n_features)\n",
    "        y_pred = self.model.predict(x_val)[0]\n",
    "        y_pred_list.extend(y_pred.tolist())\n",
    "        y_val_list.extend(y_val.tolist())\n",
    "    return pd.DataFrame({\"y\": y_val_list, \"yhat\": y_pred_list})\n",
    "\n",
    "\n",
    "ForecastLSTM.forecast_validation_dataset = forecast_validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(df_fcst: pd.DataFrame) -> dict:\n",
    "    true = df_fcst[\"y\"]\n",
    "    pred = df_fcst[\"yhat\"]\n",
    "\n",
    "    mae = (true - pred).abs().mean()\n",
    "    mape = (true - pred).abs().div(true).mean() * 100\n",
    "    mse = ((true - pred) ** 2).mean()\n",
    "    return {\n",
    "        \"mae\": mae,\n",
    "        \"mape\": mape,\n",
    "        \"mse\": mse,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('./(binary) ids_40000.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#undersampling -> normal:attack=1:1\n",
    "\n",
    "X = df.iloc[:,:23]\n",
    "y = df.iloc[:,23:24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#위 함수에서 라벨을 'y'로 설정해줬기 때문에, 데이터 셋에서 'label'을 'y'로 바꿔줌\n",
    "df.rename(columns={' Label':'y'}, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#39727*0.7 = 27808.9\n",
    "df.iloc[27808:27809,:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('Total Length of Fwd Packets')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1) Train, Test 데이터 분리\n",
    "df_train = df[df.index < cutoff]\n",
    "df_test = df[df.index >= cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2) Sequence Length, 예측 기간(Step), Single Output 여부 등 정의\n",
    "seq_len = 5  # 과거 5주의 데이터를 feature로 사용\n",
    "steps = 5  # 향후 5주의 y를 예측\n",
    "single_output = False  # 향후 5주차의 시점만이 아닌, 1~5주 모두 예측\n",
    "metrics = \"mse\"  # 모델 성능 지표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3) LSTM 하이퍼파라미터 정의\n",
    "lstm_params = {\n",
    "    \"seq_len\": seq_len,\n",
    "    \"epochs\": 100,  # epochs 반복 횟수\n",
    "    \"patience\": 30,  # early stopping 조건\n",
    "    \"steps_per_epoch\": 5,  # 1 epochs 시 dataset을 5개로 분할하여 학습\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"lstm_units\": [64, 32],  # Dense Layer: 2, Unit: (64, 32)\n",
    "    \"activation\": \"relu\",\n",
    "    \"dropout\": 0,\n",
    "    \"validation_split\": 0.3,  # 검증 데이터셋 30%\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4) 모델 훈련\n",
    "fl = ForecastLSTM()\n",
    "fl.fit_lstm(\n",
    "    df=df_train,\n",
    "    steps=steps,\n",
    "    single_output=single_output,\n",
    "    metrics=metrics,\n",
    "    **lstm_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5) Validation dataset 예측 성능\n",
    "df_fcst_val = fl.forecast_validation_dataset()\n",
    "val_loss = calculate_metrics(df_fcst=df_fcst_val)[metrics]\n",
    "print(f\"{metrics} of validation dataset: {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
